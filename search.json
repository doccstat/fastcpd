[{"path":"https://fastcpd.xingchi.li/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 fastcpd authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://fastcpd.xingchi.li/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Xingchi Li. Author, maintainer, copyright holder. Xianyang Zhang. Author, copyright holder. Trisha Dawn. Author, copyright holder.","code":""},{"path":"https://fastcpd.xingchi.li/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Zhang X, Dawn T (2023). “Sequential Gradient Descent Quasi-Newton's Method Change-Point Analysis.” Ruiz, Francisco, Dy, Jennifer, van de Meent, Jan-Willem (eds.), Proceedings 26th International Conference Artificial Intelligence Statistics, volume 206 series Proceedings Machine Learning Research, 1129–1143. https://proceedings.mlr.press/v206/zhang23b.html.","code":"@InProceedings{,   title = {Sequential Gradient Descent and Quasi-Newton's Method for Change-Point Analysis},   author = {Xianyang Zhang and Trisha Dawn},   year = {2023},   booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},   volume = {206},   pages = {1129--1143},   editor = {{Ruiz} and {Francisco} and {Dy} and {Jennifer} and {van de Meent} and {Jan-Willem}},   series = {Proceedings of Machine Learning Research},   month = {25--27 Apr},   publisher = {PMLR},   pdf = {https://proceedings.mlr.press/v206/zhang23b/zhang23b.pdf},   url = {https://proceedings.mlr.press/v206/zhang23b.html},   abstract = {One common approach to detecting change-points is minimizing a cost function over possible numbers and locations of change-points. The framework includes several well-established procedures, such as the penalized likelihood and minimum description length. Such an approach requires finding the cost value repeatedly over different segments of the data set, which can be time-consuming when (i) the data sequence is long and (ii) obtaining the cost value involves solving a non-trivial optimization problem. This paper introduces a new sequential updating method (SE) to find the cost value effectively. The core idea is to update the cost value using the information from previous steps without re-optimizing the objective function. The new method is applied to change-point detection in generalized linear models and penalized regression. Numerical studies show that the new approach can be orders of magnitude faster than the Pruned Exact Linear Time (PELT) method without sacrificing estimation accuracy.}, }"},{"path":[]},{"path":"https://fastcpd.xingchi.li/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"fastcpd (fast change point detection) fast implmentation change point detection methods R. fastcpd package designed find change points fast manner. easy install extensible kinds change point problems user specified cost function apart built-cost functions. ’d like learn use fastcpd effectively, please refer : Sequential Gradient Descent Quasi-Newton’s Method Change-Point Analysis","code":""},{"path":"https://fastcpd.xingchi.li/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"’re compiling source, can run pak::pkg_system_requirements(\"fastcpd\"), see complete set system packages needed machine. documentation & examples","code":"# Install from CRAN, (not yet available) install.packages(\"fastcpd\") # Install the development version from GitHub # install.packages(\"pak\") pak::pak(\"doccstat/fastcpd\")"},{"path":"https://fastcpd.xingchi.li/index.html","id":"dependency","dir":"","previous_headings":"","what":"Dependency","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"library(fastcpd) load following packages: Rcpp, C++ source code compilation. RcppArmadillo, fast linear algebra. fastglm, fast generalized linear models. DescTools, Winsorizing Poisson data. glmnet, penalized regression. ggplot2, data visualization.","code":""},{"path":"https://fastcpd.xingchi.li/index.html","id":"contact-us","dir":"","previous_headings":"","what":"Contact us","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"Encountered bug unintended behavior? File ticket GitHub Issues. Contact developers specified DESCRIPTION.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class to store the output created with fastcpd — fastcpd-class","title":"An S4 class to store the output created with fastcpd — fastcpd-class","text":"S4 class stores output fastcpd. fastcpd object consist several slots including call fastcpd, data used, family model, change points, cost values, residuals, estimated parameters boolean indicating whether model fitted change points change points parameters, can select using @.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class to store the output created with fastcpd — fastcpd-class","text":"call call fastcpd. data data used. family family model. cp_set change points. cost_values cost values segment. residuals residuals segment. thetas estimated parameters segment. cp_only boolean indicating whether model fitted change points change points parameters.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":null,"dir":"Reference","previous_headings":"","what":"fastcpd: A package for finding change points in an efficient way — fastcpd","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"fastcpd package provides function fastcpd find change points data set. function based paper \"Sequential Gradient Descent Quasi-Newton’s Method Change-Point Analysis\" Xianyang Zhang Trisha Dawn.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"","code":"fastcpd(   formula = y ~ . - 1,   data,   beta = NULL,   segment_count = 10,   trim = 0.025,   momentum_coef = 0,   k = function(x) 0,   family = NULL,   epsilon = 1e-10,   min_prob = 10^10,   winsorise_minval = -20,   winsorise_maxval = 20,   p = NULL,   cost = negative_log_likelihood,   cost_gradient = cost_update_gradient,   cost_hessian = cost_update_hessian,   cp_only = FALSE,   vanilla_percentage = 0 )"},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"formula symbolic description model fitted. data data frame containing data segmented. beta Initial cost value. segment_count Number segments initial guess. trim Trimming boundary change points. momentum_coef Momentum coefficient applied update. k Function number epochs SGD. family Family models. Can \"binomial\", \"poisson\", \"lasso\" \"gaussian\". provided, user must specify cost function gradient (Hessian). epsilon Epsilon avoid numerical issues. used binomial poisson. min_prob Minimum probability avoid numerical issues. used poisson. winsorise_minval Minimum value winsorised. used poisson. winsorise_maxval Maximum value winsorised. used poisson. p Number parameters estimated. cost Cost function used. specified, default negative log-likelihood corresponding family. cost_gradient Gradient custom cost function. cost_hessian Hessian custom cost function. cp_only Whether return change points cost values segment. family provided set \"custom\", parameter set true. vanilla_percentage many data processed vanilla PELT. Range 0 1. set 0, data processed sequential gradient descnet. set 1, data processed vaniall PELT. cost function explicit solution, .e. depend coefficients like mean change case, parameter set 1.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"class fastcpd object.","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":"citation","dir":"Reference","previous_headings":"","what":"CITATION","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"Zhang, Xianyang, Trisha Dawn. \"Sequential Gradient Descent Quasi-Newton's Method Change-Point Analysis.\" arXiv preprint arXiv:2210.12235 (2022).","code":""},{"path":"https://fastcpd.xingchi.li/reference/fastcpd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"","code":"# Linear regression library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(300, rep(0, p), diag(p)) theta_0 <- rbind(c(1, 1.2, -1), c(-1, 0, 0.5), c(0.5, -0.3, 0.2)) y <- c(   x[1:100, ] %*% theta_0[1, ] + rnorm(100, 0, 1),   x[101:200, ] %*% theta_0[2, ] + rnorm(100, 0, 1),   x[201:300, ] %*% theta_0[3, ] + rnorm(100, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"gaussian\" ) plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"gaussian\") #>  #> Change points: #> 98 202  #>  #> Cost values: #> 53.44023 53.1441 45.04974  #>  #> Parameters: #>    segment 1   segment 2  segment 3 #> 1  0.9704022 -1.07884004  0.5925092 #> 2  1.1786074 -0.01757927 -0.5287126 #> 3 -0.9258587  0.63906143  0.1929411  # Logistic regression library(fastcpd) set.seed(1) x <- matrix(rnorm(1500, 0, 1), ncol = 5) theta <- rbind(rnorm(5, 0, 1), rnorm(5, 2, 1)) y <- c(   rbinom(125, 1, 1 / (1 + exp(-x[1:125, ] %*% theta[1, ]))),   rbinom(175, 1, 1 / (1 + exp(-x[126:300, ] %*% theta[2, ]))) ) result <- suppressWarnings(fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"binomial\" )) summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"binomial\") #>  #> Change points: #> 126  #>  #> Cost values: #> 56.90525 30.76875  #>  #> Parameters: #>    segment 1 segment 2 #> 1  0.7259293  1.878525 #> 2 -1.0294802  2.704376 #> 3  1.0576503  3.702310 #> 4 -0.8812767  2.258796 #> 5  0.2419351  2.524173  # Poisson regression library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) delta <- rnorm(p) theta_0 <- c(1, 1.2, -1) y <- c(   rpois(300, exp(x[1:300, ] %*% theta_0)),   rpois(400, exp(x[301:700, ] %*% (theta_0 + delta))),   rpois(300, exp(x[701:1000, ] %*% theta_0)),   rpois(100, exp(x[1001:1100, ] %*% (theta_0 - delta))),   rpois(200, exp(x[1101:1300, ] %*% theta_0)),   rpois(200, exp(x[1301:1500, ] %*% (theta_0 + delta))) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   beta = (p + 1) * log(1500) / 2,   k = function(x) 0,   family = \"poisson\",   epsilon = 1e-5 ) summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     beta = (p + 1) * log(1500)/2, k = function(x) 0, family = \"poisson\",  #>     epsilon = 1e-05) #>  #> Change points: #> 329 728 1021 1107 1325  #>  #> Cost values: #> 14425.87 13971.23 697.2187 107.5353 380.7153 51.93594  #>  #> Parameters: #>     segment 1  segment 2  segment 3  segment 4 segment 5  segment 6 #> 1  2.60927673  1.9255183  0.7405125 -0.3965022  1.117753  2.5479308 #> 2  0.02398457  0.1068924  1.4721444  1.8677797  1.019035  0.4947115 #> 3 -1.34361104 -2.7353603 -0.8906937  0.4651667 -1.178933 -2.5038966  # Penalized linear regression library(fastcpd) set.seed(1) n <- 1500 p_true <- 6 p <- 50 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) theta_0 <- rbind(   runif(p_true, -5, -2),   runif(p_true, -3, 3),   runif(p_true, 2, 5),   runif(p_true, -5, 5) ) theta_0 <- cbind(theta_0, matrix(0, ncol = p - p_true, nrow = 4)) y <- c(   x[1:300, ] %*% theta_0[1, ] + rnorm(300, 0, 1),   x[301:700, ] %*% theta_0[2, ] + rnorm(400, 0, 1),   x[701:1000, ] %*% theta_0[3, ] + rnorm(300, 0, 1),   x[1001:1500, ] %*% theta_0[4, ] + rnorm(500, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"lasso\" ) plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"lasso\") #>  #> Change points: #> 300 700 1000  #>  #> Cost values: #> 202.6103 254.6904 183.5909 302.4998  #>  #> Parameters: #> 50 x 4 sparse Matrix of class \"dgCMatrix\" #>       segment 1  segment 2 segment 3  segment 4 #>  [1,] -2.892654  0.3344843  4.046784  .         #>  [2,] -2.831352 -0.3687367  3.907948  3.1109044 #>  [3,] -2.817023 -0.1990682  2.574074  2.7732194 #>  [4,] -1.926208  0.4740742  3.322680 -0.5366242 #>  [5,] -3.066517 -0.4659526  2.092768 -3.3781507 #>  [6,] -1.842233  0.4168482  4.798087  .         #>  [7,]  .         .          .         .         #>  [8,]  .         .          .         .         #>  [9,]  .         .          .         .         #> [10,]  .         .          .         .         #> [11,]  .         .          .         .         #> [12,]  .         .          .         .         #> [13,]  .         .          .         .         #> [14,]  .         .          .         .         #> [15,]  .         .          .         .         #> [16,]  .         .          .         .         #> [17,]  .         .          .         .         #> [18,]  .         .          .         .         #> [19,]  .         .          .         .         #> [20,]  .         .          .         .         #> [21,]  .         .          .         .         #> [22,]  .         .          .         .         #> [23,]  .         .          .         .         #> [24,]  .         .          .         .         #> [25,]  .         .          .         .         #> [26,]  .         .          .         .         #> [27,]  .         .          .         .         #> [28,]  .         .          .         .         #> [29,]  .         .          .         .         #> [30,]  .         .          .         .         #> [31,]  .         .          .         .         #> [32,]  .         .          .         .         #> [33,]  .         .          .         .         #> [34,]  .         .          .         .         #> [35,]  .         .          .         .         #> [36,]  .         .          .         .         #> [37,]  .         .          .         .         #> [38,]  .         .          .         .         #> [39,]  .         .          .         .         #> [40,]  .         .          .         .         #> [41,]  .         .          .         .         #> [42,]  .         .          .         .         #> [43,]  .         .          .         .         #> [44,]  .         .          .         .         #> [45,]  .         .          .         .         #> [46,]  .         .          .         .         #> [47,]  .         .          .         .         #> [48,]  .         .          .         .         #> [49,]  .         .          .         .         #> [50,]  .         .          .         .          # Custom cost function: logistic regression library(fastcpd) set.seed(1) p <- 5 x <- matrix(rnorm(375 * p, 0, 1), ncol = p) theta <- rbind(rnorm(p, 0, 1), rnorm(p, 2, 1)) y <- c(   rbinom(200, 1, 1 / (1 + exp(-x[1:200, ] %*% theta[1, ]))),   rbinom(175, 1, 1 / (1 + exp(-x[201:375, ] %*% theta[2, ]))) ) data <- data.frame(y = y, x = x) result_builtin <- fastcpd(   formula = y ~ . - 1,   data = data,   family = \"binomial\" ) #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred logistic_loss <- function(data, theta) {   x <- data[, -1]   y <- data[, 1]   u <- x %*% theta   nll <- -y * u + log(1 + exp(u))   nll[u > 10] <- -y[u > 10] * u[u > 10] + u[u > 10]   sum(nll) } logistic_loss_gradient <- function(data, theta) {   x <- data[nrow(data), -1]   y <- data[nrow(data), 1]   c(-(y - 1 / (1 + exp(-x %*% theta)))) * x } logistic_loss_hessian <- function(data, theta) {   x <- data[nrow(data), -1]   prob <- 1 / (1 + exp(-x %*% theta))   (x %o% x) * c((1 - prob) * prob) } result_custom <- fastcpd(   formula = y ~ . - 1,   data = data,   epsilon = 1e-5,   cost = logistic_loss,   cost_gradient = logistic_loss_gradient,   cost_hessian = logistic_loss_hessian ) cat(   \"Change points detected by built-in logistic regression model: \",   result_builtin@cp_set, \"\\n\",   \"Change points detected by custom logistic regression model: \",   result_custom@cp_set, \"\\n\",   sep = \"\" ) #> Change points detected by built-in logistic regression model: 200 #> Change points detected by custom logistic regression model: 201  # Custom cost function: mean shift library(fastcpd) set.seed(1) p <- 1 data <- rbind(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(400, mean = rep(50, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(300, mean = rep(2, p), sigma = diag(100, p)) ) segment_count_guess <- 10 block_size <- max(floor(sqrt(nrow(data)) / (segment_count_guess + 1)), 2) block_count <- floor(nrow(data) / block_size) data_all_vars <- rep(0, block_count) for (block_index in seq_len(block_count)) {   block_start <- (block_index - 1) * block_size + 1   block_end <- if (block_index < block_count) block_index * block_size else nrow(data)   data_all_vars[block_index] <- var(data[block_start:block_end, ]) } data_all_var <- mean(data_all_vars) mean_loss <- function(data) {   n <- nrow(data)   (norm(data, type = \"F\")^2 - colSums(data)^2 / n) / 2 / data_all_var +     n / 2 * (log(data_all_var) + log(2 * pi)) } mean_loss_result <- fastcpd(   formula = ~ . - 1,   data = data.frame(data),   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = mean_loss ) summary(mean_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data.frame(data), beta = (p +  #>     1) * log(nrow(data))/2, p = p, cost = mean_loss) #>  #> Change points: #> 300 700   # Custom cost function: variance change library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(2, p)) ) data_all_mu <- colMeans(data) var_loss <- function(data) {   demeaned_data_norm <- norm(sweep(data, 2, data_all_mu), type = \"F\")   nrow(data) * (1 + log(2 * pi) + log(demeaned_data_norm^2 / nrow(data))) / 2 } var_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = var_loss ) summary(var_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (p + 1) * log(nrow(data))/2,  #>     p = p, cost = var_loss) #>  #> Change points: #> 300 699   # Custom cost function: mean shift and variance change library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(10, p), sigma = diag(50, p)) ) meanvar_loss <- function(data) {   loss_part <- (colSums(data^2) - colSums(data)^2 / nrow(data)) / nrow(data)   nrow(data) * (1 + log(2 * pi) + log(loss_part)) / 2 } meanvar_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (2 * p + 1) * log(nrow(data)) / 2,   p = 2 * p,   cost = meanvar_loss ) summary(meanvar_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (2 * p + 1) * log(nrow(data))/2,  #>     p = 2 * p, cost = meanvar_loss) #>  #> Change points: #> 300 700 1000 1300 1700   # Custom cost function: Huber loss library(fastcpd) set.seed(1) n <- 400 + 300 + 500 p <- 5 x <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = diag(p)) theta <- rbind(   mvtnorm::rmvnorm(1, mean = rep(0, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(5, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(9, p - 3), sigma = diag(p - 3)) ) theta <- cbind(theta, matrix(0, 3, 3)) theta <- theta[rep(seq_len(3), c(400, 300, 500)), ] y_true <- rowSums(x * theta) factor <- c(   2 * stats::rbinom(400, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(300, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(500, size = 1, prob = 0.95) - 1 ) y <- factor * y_true + stats::rnorm(n) data <- cbind.data.frame(y, x) huber_threshold <- 1 huber_loss <- function(data, theta) {   residual <- data[, 1] - data[, -1, drop = FALSE] %*% theta   indicator <- abs(residual) <= huber_threshold   sum(     residual^2 / 2 * indicator +       huber_threshold * (abs(residual) - huber_threshold / 2) * (1 - indicator)   ) } huber_loss_gradient <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     -residual * data[nrow(data), -1]   } else {     -huber_threshold * sign(residual) * data[nrow(data), -1]   } } huber_loss_hessian <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     outer(data[nrow(data), -1], data[nrow(data), -1])   } else {     0.01 * diag(length(theta))   } } huber_regression_result <- fastcpd(   formula = y ~ . - 1,   data = data,   beta = (p + 1) * log(n) / 2,   cost = huber_loss,   cost_gradient = huber_loss_gradient,   cost_hessian = huber_loss_hessian ) summary(huber_regression_result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data, beta = (p + 1) * log(n)/2,  #>     cost = huber_loss, cost_gradient = huber_loss_gradient, cost_hessian = huber_loss_hessian) #>  #> Change points: #> 401 726"},{"path":"https://fastcpd.xingchi.li/reference/plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the data and the change points for a fastcpd object — plot.fastcpd","title":"Plot the data and the change points for a fastcpd object — plot.fastcpd","text":"Plot data change points fastcpd object","code":""},{"path":"https://fastcpd.xingchi.li/reference/plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the data and the change points for a fastcpd object — plot.fastcpd","text":"","code":"# S3 method for fastcpd plot(x, ...)  # S4 method for fastcpd,missing plot(x, y, ...)"},{"path":"https://fastcpd.xingchi.li/reference/plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the data and the change points for a fastcpd object — plot.fastcpd","text":"x fastcpd object. ... Ignored. y Ignored.","code":""},{"path":"https://fastcpd.xingchi.li/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the call and the change points for a fastcpd object — print.fastcpd","title":"Print the call and the change points for a fastcpd object — print.fastcpd","text":"Print call change points fastcpd object","code":""},{"path":"https://fastcpd.xingchi.li/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the call and the change points for a fastcpd object — print.fastcpd","text":"","code":"# S3 method for fastcpd print(x, ...)  # S4 method for fastcpd print(x, ...)"},{"path":"https://fastcpd.xingchi.li/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the call and the change points for a fastcpd object — print.fastcpd","text":"x fastcpd object. ... Ignored.","code":""},{"path":"https://fastcpd.xingchi.li/reference/show.html","id":null,"dir":"Reference","previous_headings":"","what":"Show the available methods for a fastcpd object — show.fastcpd","title":"Show the available methods for a fastcpd object — show.fastcpd","text":"Show available methods fastcpd object","code":""},{"path":"https://fastcpd.xingchi.li/reference/show.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show the available methods for a fastcpd object — show.fastcpd","text":"","code":"# S3 method for fastcpd show(object)  # S4 method for fastcpd show(object)"},{"path":"https://fastcpd.xingchi.li/reference/show.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show the available methods for a fastcpd object — show.fastcpd","text":"object fastcpd object.","code":""},{"path":"https://fastcpd.xingchi.li/reference/summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Show the summary of a fastcpd object — summary.fastcpd","title":"Show the summary of a fastcpd object — summary.fastcpd","text":"Show summary fastcpd object","code":""},{"path":"https://fastcpd.xingchi.li/reference/summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show the summary of a fastcpd object — summary.fastcpd","text":"","code":"# S3 method for fastcpd summary(object, ...)  # S4 method for fastcpd summary(object, ...)"},{"path":"https://fastcpd.xingchi.li/reference/summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show the summary of a fastcpd object — summary.fastcpd","text":"object fastcpd object. ... Ignored.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-033","dir":"Changelog","previous_headings":"","what":"fastcpd 0.3.3","title":"fastcpd 0.3.3","text":"Merge implementation vanilla PELT SeN. Encapsulate implementation binding new coefficients previous coefficients. Rewrite fastcpd parameters updating C++.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-032","dir":"Changelog","previous_headings":"","what":"fastcpd 0.3.2","title":"fastcpd 0.3.2","text":"Integrate initialization update theta_hat, theta_sum hessian. Combine theta estimation single function. Add parameter vanilla_percentage denote method switching vanilla PETL SeN. Add documentation cp_only parameter. Add preparation merging vanilla PELT SeN.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-031","dir":"Changelog","previous_headings":"","what":"fastcpd 0.3.1","title":"fastcpd 0.3.1","text":"Add examples tests fastcpd. Rearrange C++ functions. Add precondition check.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-030","dir":"Changelog","previous_headings":"","what":"fastcpd 0.3.0","title":"fastcpd 0.3.0","text":"Bump test coverage class methods fastcpd.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-029","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.9","title":"fastcpd 0.2.9","text":"Fix Poisson regression bug related lfactorial.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-028","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.8","title":"fastcpd 0.2.8","text":"Make penalized linear regression estimated coefficients output sparse.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-027","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.7","title":"fastcpd 0.2.7","text":"Fix mean change example bug. Update documentation redirect README pkgdown generated webpage. Add contact methods ways file ticket.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-026","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.6","title":"fastcpd 0.2.6","text":"Add C++ sanity check Logistic regression data, .e. binomial family. Add examples tests fastcpd. Rename C++ source files follow Unix convention. Update documentation link README.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-025","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.5","title":"fastcpd 0.2.5","text":"Hide internal functions documentation. Export fastcpd class.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-024","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.4","title":"fastcpd 0.2.4","text":"Add column name thetas slot fastcpd class. Fix plot residuals responses appear plot. Default cp_only FALSE. Remove residuals summary method.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-023","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.3","title":"fastcpd 0.2.3","text":"Add missing examples linear regression LASSO.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-022","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.2","title":"fastcpd 0.2.2","text":"Add examples illustrate use fastcpd function. Indicating internal functions users use .","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-021","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.1","title":"fastcpd 0.2.1","text":"Add examples README.","code":""},{"path":"https://fastcpd.xingchi.li/news/index.html","id":"fastcpd-020","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.0","title":"fastcpd 0.2.0","text":"Added NEWS.md file track changes package.","code":""}]
