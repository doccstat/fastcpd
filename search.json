[{"path":"http://www.xingchi.li/fastcpd/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 fastcpd authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"http://www.xingchi.li/fastcpd/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Xingchi Li. Author, maintainer, copyright holder. Xianyang Zhang. Author, copyright holder. Trisha Dawn. Author, copyright holder.","code":""},{"path":"http://www.xingchi.li/fastcpd/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Zhang X, Dawn T (2023). “Sequential Gradient Descent Quasi-Newton's Method Change-Point Analysis.” Ruiz, Francisco, Dy, Jennifer, van de Meent, Jan-Willem (eds.), Proceedings 26th International Conference Artificial Intelligence Statistics, volume 206 series Proceedings Machine Learning Research, 1129–1143. https://proceedings.mlr.press/v206/zhang23b.html.","code":"@InProceedings{,   title = {Sequential Gradient Descent and Quasi-Newton's Method for Change-Point Analysis},   author = {Xianyang Zhang and Trisha Dawn},   year = {2023},   booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},   volume = {206},   pages = {1129--1143},   editor = {{Ruiz} and {Francisco} and {Dy} and {Jennifer} and {van de Meent} and {Jan-Willem}},   series = {Proceedings of Machine Learning Research},   month = {25--27 Apr},   publisher = {PMLR},   pdf = {https://proceedings.mlr.press/v206/zhang23b/zhang23b.pdf},   url = {https://proceedings.mlr.press/v206/zhang23b.html},   abstract = {One common approach to detecting change-points is minimizing a cost function over possible numbers and locations of change-points. The framework includes several well-established procedures, such as the penalized likelihood and minimum description length. Such an approach requires finding the cost value repeatedly over different segments of the data set, which can be time-consuming when (i) the data sequence is long and (ii) obtaining the cost value involves solving a non-trivial optimization problem. This paper introduces a new sequential updating method (SE) to find the cost value effectively. The core idea is to update the cost value using the information from previous steps without re-optimizing the objective function. The new method is applied to change-point detection in generalized linear models and penalized regression. Numerical studies show that the new approach can be orders of magnitude faster than the Pruned Exact Linear Time (PELT) method without sacrificing estimation accuracy.}, }"},{"path":[]},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"fastcpd (FAST Change Point Detection) fast implmentation change point detection methods R. fastcpd package designed find change points fast manner. easy install extensible kinds change point problems user specified cost function apart built-cost functions. ’d like learn use fastcpd effectively, please refer following references: Sequential Gradient Descent Quasi-Newton’s Method Change-Point Analysis","code":""},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"’re compiling source, can run pak::pkg_system_requirements(\"fastcpd\"), see complete set system packages needed machine.","code":"# Install from CRAN, (not yet available) install.packages(\"fastcpd\") # Install the development version from GitHub # install.packages(\"pak\") pak::pak(\"doccstat/fastcpd\")"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"library(fastcpd) load core fastcpd package full part: Rcpp, C++ source code compilation. RcppArmadillo, fast linear algebra. fastglm, fast generalized linear models. DescTools, Winsorizing Poisson data. glmnet, penalized regression. ggplot2, data visualization.","code":""},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"documentation","dir":"","previous_headings":"Usage","what":"Documentation","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"https://www.xingchi.li/fastcpd","code":""},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"linear-regression","dir":"","previous_headings":"Usage","what":"Linear regression","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(300, rep(0, p), diag(p)) theta_0 <- rbind(c(1, 1.2, -1), c(-1, 0, 0.5), c(0.5, -0.3, 0.2)) y <- c(   x[1:100, ] %*% theta_0[1, ] + rnorm(100, 0, 1),   x[101:200, ] %*% theta_0[2, ] + rnorm(100, 0, 1),   x[201:300, ] %*% theta_0[3, ] + rnorm(100, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"gaussian\",   cp_only = FALSE ) plot(result) summary(result) #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x), #>     family = \"gaussian\", cp_only = FALSE) #> #> Change points: #> 98 202"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"logistic-regression","dir":"","previous_headings":"Usage","what":"Logistic regression","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) x <- matrix(rnorm(1500, 0, 1), ncol = 5) theta <- rbind(rnorm(5, 0, 1), rnorm(5, 2, 1)) y <- c(   rbinom(125, 1, 1 / (1 + exp(-x[1:125, ] %*% theta[1, ]))),   rbinom(175, 1, 1 / (1 + exp(-x[126:300, ] %*% theta[2, ]))) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"binomial\",   cp_only = FALSE ) plot(result) summary(result) #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x), #>     family = \"binomial\", cp_only = FALSE) #> #> Residuals: #>       Min        1Q    Median        3Q       Max #> -14.09576  -1.07218  -1.00000   1.07353  35.39472 #> #> Change points: #> 126"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"poisson-regression","dir":"","previous_headings":"Usage","what":"Poisson regression","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) delta <- rnorm(p) theta_0 <- c(1, 1.2, -1) y <- c(   rpois(300, exp(x[1:300, ] %*% theta_0)),   rpois(400, exp(x[301:700, ] %*% (theta_0 + delta))),   rpois(300, exp(x[701:1000, ] %*% theta_0)),   rpois(100, exp(x[1001:1100, ] %*% (theta_0 - delta))),   rpois(200, exp(x[1101:1300, ] %*% theta_0)),   rpois(200, exp(x[1301:1500, ] %*% (theta_0 + delta))) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   beta = (p + 1) * log(1500) / 2,   k = function(x) 0,   family = \"poisson\",   epsilon = 1e-5,   cp_only = FALSE ) plot(result) summary(result) #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x), #>     beta = (p + 1) * log(1500)/2, k = function(x) 0, family = \"poisson\", #>     epsilon = 1e-05, cp_only = FALSE) #> #> Residuals: #>       Min        1Q    Median        3Q       Max #>   -1.0000   -1.0000   -0.5785    0.3564 1793.2299 #> #> Change points: #> 329 728 1021 1107 1325"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"penalized-linear-regression","dir":"","previous_headings":"Usage","what":"Penalized linear regression","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) n <- 1500 p_true <- 6 p <- 50 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) theta_0 <- rbind(   runif(p_true, -5, -2),   runif(p_true, -3, 3),   runif(p_true, 2, 5),   runif(p_true, -5, 5) ) theta_0 <- cbind(theta_0, matrix(0, ncol = p - p_true, nrow = 4)) y <- c(   x[1:300, ] %*% theta_0[1, ] + rnorm(300, 0, 1),   x[301:700, ] %*% theta_0[2, ] + rnorm(400, 0, 1),   x[701:1000, ] %*% theta_0[3, ] + rnorm(300, 0, 1),   x[1001:1500, ] %*% theta_0[4, ] + rnorm(500, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"lasso\",   cp_only = FALSE ) plot(result) summary(result) #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x), #>     family = \"lasso\", cp_only = FALSE) #> #> Change points: #> 300 700 1000"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"custom-cost-function-mean-shift","dir":"","previous_headings":"Usage","what":"Custom cost function: mean shift","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) p <- 1 data <- rbind(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(400, mean = rep(50, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(300, mean = rep(2, p), sigma = diag(100, p)) ) segment_count_guess <- 10 block_size <- max(floor(sqrt(nrow(data)) / (segment_count_guess + 1)), 2) block_count <- ceiling(nrow(data) / block_size) data_all_vars <- rep(0, block_count) for (block_index in seq_len(block_count)) {   block_start <- (block_index - 1) * block_size + 1   block_end <- min(block_index * block_size, nrow(data))   data_all_vars[block_index] <- var(data[block_start:block_end, ]) } data_all_var <- mean(data_all_vars) mean_loss <- function(data) {   n <- nrow(data)   (norm(data, type = \"F\") ^ 2 - colSums(data) ^ 2 / n) / 2 / data_all_var +     n / 2 * (log(data_all_var) + log(2 * pi)) } mean_loss_result <- fastcpd(   formula = ~ . - 1,   data = data.frame(data),   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = mean_loss ) summary(mean_loss_result) #> Call: #> fastcpd(formula = ~. - 1, data = data.frame(data), beta = (p + #>     1) * log(nrow(data))/2, p = p, cost = mean_loss) #> #> Change points: #> 300 700"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"custom-cost-function-variance-change","dir":"","previous_headings":"Usage","what":"Custom cost function: variance change","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(2, p)) ) data_all_mu <- colMeans(data) var_loss <- function(data) {   demeaned_data_norm <- norm(sweep(data, 2, data_all_mu), type = \"F\")   nrow(data) * (1 + log(2 * pi) + log(demeaned_data_norm ^ 2 / nrow(data))) / 2 } var_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = var_loss ) summary(var_loss_result) #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (p + 1) * log(nrow(data))/2, #>     p = p, cost = var_loss) #> #> Change points: #> 300 699"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"custom-cost-function-mean-shift-and-variance-change","dir":"","previous_headings":"Usage","what":"Custom cost function: mean shift and variance change","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(10, p), sigma = diag(50, p)) ) meanvar_loss <- function(data) {   loss_part <- (colSums(data ^ 2) - colSums(data) ^ 2 / nrow(data)) / nrow(data)   nrow(data) * (1 + log(2 * pi) + log(loss_part)) / 2 } meanvar_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (2 * p + 1) * log(nrow(data)) / 2,   p = 2 * p,   cost = meanvar_loss ) summary(meanvar_loss_result) #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (2 * p + 1) * log(nrow(data))/2, #>     p = 2 * p, cost = meanvar_loss) #> #> Change points: #> 300 700 1000 1300 1700"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"custom-cost-function-huber-loss","dir":"","previous_headings":"Usage","what":"Custom cost function: Huber loss","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"","code":"library(fastcpd) set.seed(1) n <- 500 + 700 + 500 p <- 8 x <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = diag(p)) theta <- rbind(   mvtnorm::rmvnorm(1, mean = rep(0, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(5, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(9, p - 3), sigma = diag(p - 3)) ) theta <- cbind(theta, matrix(0, 3, 3)) theta <- theta[rep(seq_len(3), c(500, 700, 500)), ] y_true <- rowSums(x * theta) factor <- c(   2 * stats::rbinom(500, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(700, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(500, size = 1, prob = 0.95) - 1 ) y <- factor * y_true + stats::rnorm(n) data <- cbind.data.frame(y, x) huber_threshold <- 1 huber_loss <- function(data, theta) {   residual <- data[, 1] - data[, -1, drop = FALSE] %*% theta   indicator <- abs(residual) <= huber_threshold   sum(     residual ^ 2 / 2 * indicator +     huber_threshold * (abs(residual) - huber_threshold / 2) * (1 - indicator)   ) } huber_loss_gradient <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     - residual * data[nrow(data), -1]   } else {     - huber_threshold * sign(residual) * data[nrow(data), -1]   } } huber_loss_hessian <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     outer(data[nrow(data), -1], data[nrow(data), -1])   } else {     0.01 * diag(length(theta))   } } huber_regression_result <- fastcpd(   formula = y ~ . - 1,   data = data,   beta = (p + 1) * log(n) / 2,   cost = huber_loss,   cost_gradient = huber_loss_gradient,   cost_hessian = huber_loss_hessian ) summary(huber_regression_result) #> Call: #> fastcpd(formula = y ~ . - 1, data = data, beta = (p + 1) * log(n)/2, #>     cost = huber_loss, cost_gradient = huber_loss_gradient, cost_hessian = huber_loss_hessian) #> #> Change points: #> 575 1215 1395"},{"path":"http://www.xingchi.li/fastcpd/index.html","id":"todo","dir":"","previous_headings":"","what":"TODO","title":"Fast Change Point Detection Based on Dynamic Programming with Pruning\n    with Sequential Gradient Descent","text":"examples roxygen docs multivariate custom cost functions, pkgdown references","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the cost values for the segmentation.\nThis function is not meant to be called directly by the user. — cost_update","title":"Update the cost values for the segmentation.\nThis function is not meant to be called directly by the user. — cost_update","text":"Update cost values segmentation. function meant called directly user.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the cost values for the segmentation.\nThis function is not meant to be called directly by the user. — cost_update","text":"","code":"cost_update(   data,   theta_hat,   theta_sum,   hessian,   tau,   i,   k,   family,   momentum,   momentum_coef,   epsilon,   min_prob,   winsorise_minval,   winsorise_maxval,   lambda,   cost_gradient,   cost_hessian )"},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the cost values for the segmentation.\nThis function is not meant to be called directly by the user. — cost_update","text":"data data frame containing data segmented. theta_hat Estimated theta previous iteration. theta_sum Sum estimated theta previous iteration. hessian Hessian matrix previous iteration. tau Start current segment. Index current data whole data set. k Number epochs SGD. family Family model. momentum Momentum previous iteration. momentum_coef Momentum coefficient applied current momentum. epsilon Epsilon avoid numerical issues. used binomial poisson. min_prob Minimum probability avoid numerical issues. used poisson. winsorise_minval Minimum value winsorised. used poisson. winsorise_maxval Maximum value winsorised. used poisson. lambda Lambda L1 regularization. used lasso. cost_gradient Gradient custom cost function. cost_hessian Hessian custom cost function.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update the cost values for the segmentation.\nThis function is not meant to be called directly by the user. — cost_update","text":"list containing new values theta_hat, theta_sum,  hessian, momentum.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_gradient.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to calculate the gradient at the current data.\nThis function is not meant to be called directly by the user. — cost_update_gradient","title":"Function to calculate the gradient at the current data.\nThis function is not meant to be called directly by the user. — cost_update_gradient","text":"Function calculate gradient current data. function meant called directly user.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_gradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to calculate the gradient at the current data.\nThis function is not meant to be called directly by the user. — cost_update_gradient","text":"","code":"cost_update_gradient(data, theta, family)"},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_gradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to calculate the gradient at the current data.\nThis function is not meant to be called directly by the user. — cost_update_gradient","text":"data data frame containing data segmented. theta Estimated theta previous iteration. family Family model.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_gradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to calculate the gradient at the current data.\nThis function is not meant to be called directly by the user. — cost_update_gradient","text":"Gradient current data.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_hessian.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to calculate the Hessian matrix at the current data.\nThis function is not meant to be called directly by the user. — cost_update_hessian","title":"Function to calculate the Hessian matrix at the current data.\nThis function is not meant to be called directly by the user. — cost_update_hessian","text":"Function calculate Hessian matrix current data. function meant called directly user.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_hessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to calculate the Hessian matrix at the current data.\nThis function is not meant to be called directly by the user. — cost_update_hessian","text":"","code":"cost_update_hessian(data, theta, family, min_prob)"},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_hessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to calculate the Hessian matrix at the current data.\nThis function is not meant to be called directly by the user. — cost_update_hessian","text":"data data frame containing data segmented. theta Estimated theta previous iteration. family Family model. min_prob Minimum probability avoid numerical issues.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/cost_update_hessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to calculate the Hessian matrix at the current data.\nThis function is not meant to be called directly by the user. — cost_update_hessian","text":"Hessian current data.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd-class.html","id":null,"dir":"Reference","previous_headings":"","what":"An S4 class to store the output created with fastcpd — fastcpd-class","title":"An S4 class to store the output created with fastcpd — fastcpd-class","text":"S4 class stores output fastcpd. fastcpd object consist several slots including call fastcpd, data used, family model, change points, cost values, residuals, estimated parameters boolean indicating whether model fitted change points change points parameters, can select using @.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"An S4 class to store the output created with fastcpd — fastcpd-class","text":"call call fastcpd. data data used. family family model. cp_set change points. cost_values cost values segment. residuals residuals segment. thetas estimated parameters segment. cp_only boolean indicating whether model fitted change points change points parameters.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":null,"dir":"Reference","previous_headings":"","what":"fastcpd: A package for finding change points in an efficient way — fastcpd","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"fastcpd package provides function fastcpd find change points data set. function based paper \"Sequential Gradient Descent Quasi-Newton’s Method Change-Point Analysis\" Xianyang Zhang Trisha Dawn.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"","code":"fastcpd(   formula = y ~ . - 1,   data,   beta = NULL,   segment_count = 10,   trim = 0.025,   momentum_coef = 0,   k = function(x) 0,   family = NULL,   epsilon = 1e-10,   min_prob = 10^10,   winsorise_minval = -20,   winsorise_maxval = 20,   p = NULL,   cost = negative_log_likelihood,   cost_gradient = cost_update_gradient,   cost_hessian = cost_update_hessian,   cp_only = TRUE )"},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"formula symbolic description model fitted. data data frame containing data segmented. beta Initial cost value. segment_count Number segments initial guess. trim Trimming boundary change points. momentum_coef Momentum coefficient applied update. k Function number epochs SGD. family Family models. Can \"binomial\", \"poisson\", \"lasso\" \"gaussian\". provided, user must specify cost function gradient (Hessian). epsilon Epsilon avoid numerical issues. used binomial poisson. min_prob Minimum probability avoid numerical issues. used poisson. winsorise_minval Minimum value winsorised. used poisson. winsorise_maxval Maximum value winsorised. used poisson. p Number parameters estimated. cost Cost function used. specified, default negative log-likelihood corresponding family. cost_gradient Gradient custom cost function. cost_hessian Hessian custom cost function. cp_only Whether return change points cost values segment.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"class fastcpd object.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":"citation","dir":"Reference","previous_headings":"","what":"CITATION","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"Zhang, Xianyang, Trisha Dawn. \"Sequential Gradient Descent Quasi-Newton's Method Change-Point Analysis.\" arXiv preprint arXiv:2210.12235 (2022).","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"fastcpd: A package for finding change points in an efficient way — fastcpd","text":"","code":"# Linear regression library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(300, rep(0, p), diag(p)) theta_0 <- rbind(c(1, 1.2, -1), c(-1, 0, 0.5), c(0.5, -0.3, 0.2)) y <- c(   x[1:100, ] %*% theta_0[1, ] + rnorm(100, 0, 1),   x[101:200, ] %*% theta_0[2, ] + rnorm(100, 0, 1),   x[201:300, ] %*% theta_0[3, ] + rnorm(100, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"gaussian\",   cp_only = FALSE ) plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"gaussian\", cp_only = FALSE) #>  #> Change points: #> 98 202  #>   # Logistic regression library(fastcpd) set.seed(1) x <- matrix(rnorm(1500, 0, 1), ncol = 5) theta <- rbind(rnorm(5, 0, 1), rnorm(5, 2, 1)) y <- c(   rbinom(125, 1, 1 / (1 + exp(-x[1:125, ] %*% theta[1, ]))),   rbinom(175, 1, 1 / (1 + exp(-x[126:300, ] %*% theta[2, ]))) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"binomial\",   cp_only = FALSE ) #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred #> Warning: fit_glm: fitted probabilities numerically 0 or 1 occurred plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"binomial\", cp_only = FALSE) #>  #> Change points: #> 126  #>   # Poisson regression library(fastcpd) set.seed(1) p <- 3 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) delta <- rnorm(p) theta_0 <- c(1, 1.2, -1) y <- c(   rpois(300, exp(x[1:300, ] %*% theta_0)),   rpois(400, exp(x[301:700, ] %*% (theta_0 + delta))),   rpois(300, exp(x[701:1000, ] %*% theta_0)),   rpois(100, exp(x[1001:1100, ] %*% (theta_0 - delta))),   rpois(200, exp(x[1101:1300, ] %*% theta_0)),   rpois(200, exp(x[1301:1500, ] %*% (theta_0 + delta))) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   beta = (p + 1) * log(1500) / 2,   k = function(x) 0,   family = \"poisson\",   epsilon = 1e-5,   cp_only = FALSE ) plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     beta = (p + 1) * log(1500)/2, k = function(x) 0, family = \"poisson\",  #>     epsilon = 1e-05, cp_only = FALSE) #>  #> Change points: #> 329 728 1021 1107 1325  #>   # Penalized linear regression library(fastcpd) set.seed(1) n <- 1500 p_true <- 6 p <- 50 x <- mvtnorm::rmvnorm(1500, rep(0, p), diag(p)) theta_0 <- rbind(   runif(p_true, -5, -2),   runif(p_true, -3, 3),   runif(p_true, 2, 5),   runif(p_true, -5, 5) ) theta_0 <- cbind(theta_0, matrix(0, ncol = p - p_true, nrow = 4)) y <- c(   x[1:300, ] %*% theta_0[1, ] + rnorm(300, 0, 1),   x[301:700, ] %*% theta_0[2, ] + rnorm(400, 0, 1),   x[701:1000, ] %*% theta_0[3, ] + rnorm(300, 0, 1),   x[1001:1500, ] %*% theta_0[4, ] + rnorm(500, 0, 1) ) result <- fastcpd(   formula = y ~ . - 1,   data = data.frame(y = y, x = x),   family = \"lasso\",   cp_only = FALSE ) plot(result)  summary(result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data.frame(y = y, x = x),  #>     family = \"lasso\", cp_only = FALSE) #>  #> Change points: #> 300 700 1000  #>   # Custom cost function: mean shift library(fastcpd) set.seed(1) p <- 1 data <- rbind(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(400, mean = rep(50, p), sigma = diag(100, p)),   mvtnorm::rmvnorm(300, mean = rep(2, p), sigma = diag(100, p)) ) segment_count_guess <- 10 block_size <- max(floor(sqrt(nrow(data)) / (segment_count_guess + 1)), 2) block_count <- ceiling(nrow(data) / block_size) data_all_vars <- rep(0, block_count) for (block_index in seq_len(block_count)) {   block_start <- (block_index - 1) * block_size + 1   block_end <- min(block_index * block_size, nrow(data))   data_all_vars[block_index] <- var(data[block_start:block_end, ]) } data_all_var <- mean(data_all_vars) mean_loss <- function(data) {   n <- nrow(data)   (norm(data, type = \"F\") ^ 2 - colSums(data) ^ 2 / n) / 2 / data_all_var +     n / 2 * (log(data_all_var) + log(2 * pi)) } mean_loss_result <- fastcpd(   formula = ~ . - 1,   data = data.frame(data),   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = mean_loss ) summary(mean_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data.frame(data), beta = (p +  #>     1) * log(nrow(data))/2, p = p, cost = mean_loss) #>  #> Change points: #> 300 700  #>   # Custom cost function: variance change library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(2, p)) ) data_all_mu <- colMeans(data) var_loss <- function(data) {   demeaned_data_norm <- norm(sweep(data, 2, data_all_mu), type = \"F\")   nrow(data) * (1 + log(2 * pi) + log(demeaned_data_norm ^ 2 / nrow(data))) / 2 } var_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (p + 1) * log(nrow(data)) / 2,   p = p,   cost = var_loss ) summary(var_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (p + 1) * log(nrow(data))/2,  #>     p = p, cost = var_loss) #>  #> Change points: #> 300 699  #>   # Custom cost function: mean shift and variance change library(fastcpd) set.seed(1) p <- 1 data <- rbind.data.frame(   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(50, p)),   mvtnorm::rmvnorm(300, mean = rep(0, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(400, mean = rep(10, p), sigma = diag(1, p)),   mvtnorm::rmvnorm(300, mean = rep(10, p), sigma = diag(50, p)) ) meanvar_loss <- function(data) {   loss_part <- (colSums(data ^ 2) - colSums(data) ^ 2 / nrow(data)) / nrow(data)   nrow(data) * (1 + log(2 * pi) + log(loss_part)) / 2 } meanvar_loss_result <- fastcpd(   formula = ~ . - 1,   data = data,   beta = (2 * p + 1) * log(nrow(data)) / 2,   p = 2 * p,   cost = meanvar_loss ) summary(meanvar_loss_result) #>  #> Call: #> fastcpd(formula = ~. - 1, data = data, beta = (2 * p + 1) * log(nrow(data))/2,  #>     p = 2 * p, cost = meanvar_loss) #>  #> Change points: #> 300 700 1000 1300 1700  #>   # Custom cost function: Huber loss library(fastcpd) set.seed(1) n <- 500 + 700 + 500 p <- 8 x <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = diag(p)) theta <- rbind(   mvtnorm::rmvnorm(1, mean = rep(0, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(5, p - 3), sigma = diag(p - 3)),   mvtnorm::rmvnorm(1, mean = rep(9, p - 3), sigma = diag(p - 3)) ) theta <- cbind(theta, matrix(0, 3, 3)) theta <- theta[rep(seq_len(3), c(500, 700, 500)), ] y_true <- rowSums(x * theta) factor <- c(   2 * stats::rbinom(500, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(700, size = 1, prob = 0.95) - 1,   2 * stats::rbinom(500, size = 1, prob = 0.95) - 1 ) y <- factor * y_true + stats::rnorm(n) data <- cbind.data.frame(y, x) huber_threshold <- 1 huber_loss <- function(data, theta) {   residual <- data[, 1] - data[, -1, drop = FALSE] %*% theta   indicator <- abs(residual) <= huber_threshold   sum(     residual ^ 2 / 2 * indicator +     huber_threshold * (abs(residual) - huber_threshold / 2) * (1 - indicator)   ) } huber_loss_gradient <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     - residual * data[nrow(data), -1]   } else {     - huber_threshold * sign(residual) * data[nrow(data), -1]   } } huber_loss_hessian <- function(data, theta) {   residual <- c(data[nrow(data), 1] - data[nrow(data), -1] %*% theta)   if (abs(residual) <= huber_threshold) {     outer(data[nrow(data), -1], data[nrow(data), -1])   } else {     0.01 * diag(length(theta))   } } huber_regression_result <- fastcpd(   formula = y ~ . - 1,   data = data,   beta = (p + 1) * log(n) / 2,   cost = huber_loss,   cost_gradient = huber_loss_gradient,   cost_hessian = huber_loss_hessian ) summary(huber_regression_result) #>  #> Call: #> fastcpd(formula = y ~ . - 1, data = data, beta = (p + 1) * log(n)/2,  #>     cost = huber_loss, cost_gradient = huber_loss_gradient, cost_hessian = huber_loss_hessian) #>  #> Change points: #> 575 1215 1395  #>"},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd_vanilla.html","id":null,"dir":"Reference","previous_headings":"","what":"Vanilla PELT implementation.\nThis function is not meant to be called directly by the user. — fastcpd_vanilla","title":"Vanilla PELT implementation.\nThis function is not meant to be called directly by the user. — fastcpd_vanilla","text":"Vanilla PELT implementation. function meant called directly user.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd_vanilla.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vanilla PELT implementation.\nThis function is not meant to be called directly by the user. — fastcpd_vanilla","text":"","code":"fastcpd_vanilla(   data,   beta,   segment_count,   trim,   momentum_coef,   k,   family,   epsilon,   min_prob,   winsorise_minval,   winsorise_maxval,   p,   cost,   cp_only )"},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd_vanilla.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vanilla PELT implementation.\nThis function is not meant to be called directly by the user. — fastcpd_vanilla","text":"data data frame containing data segmented. beta Initial cost value. segment_count Number segments initial guess. trim Trimming boundary change points. momentum_coef Momentum coefficient applied update. k Function number epochs SGD. family Family model. Can \"binomial\", \"poisson\", \"lasso\" \"gaussian\". provided, user must specify cost function gradient (Hessian). epsilon Epsilon avoid numerical issues. used binomial poisson. min_prob Minimum probability avoid numerical issues. used poisson. winsorise_minval Minimum value winsorised. used poisson. winsorise_maxval Maximum value winsorised. used poisson. p Number parameters estimated. cost Cost function used. specified, default negative log-likelihood corresponding family. cp_only Whether return change points cost values segment.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/fastcpd_vanilla.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Vanilla PELT implementation.\nThis function is not meant to be called directly by the user. — fastcpd_vanilla","text":"Change points corresponding cost values.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/negative_log_likelihood.html","id":null,"dir":"Reference","previous_headings":"","what":"Solve logistic/poisson regression using Gradient Descent Extension to the\nmultivariate case\nThis function is not meant to be called directly by the user. — negative_log_likelihood","title":"Solve logistic/poisson regression using Gradient Descent Extension to the\nmultivariate case\nThis function is not meant to be called directly by the user. — negative_log_likelihood","text":"Solve logistic/poisson regression using Gradient Descent Extension multivariate case function meant called directly user.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/negative_log_likelihood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Solve logistic/poisson regression using Gradient Descent Extension to the\nmultivariate case\nThis function is not meant to be called directly by the user. — negative_log_likelihood","text":"","code":"negative_log_likelihood(data, theta, family, lambda, cv = FALSE)"},{"path":"http://www.xingchi.li/fastcpd/reference/negative_log_likelihood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Solve logistic/poisson regression using Gradient Descent Extension to the\nmultivariate case\nThis function is not meant to be called directly by the user. — negative_log_likelihood","text":"data data frame containing data segmented. theta Estimate parameters. null, function estimate parameters. family Family model. lambda Lambda L1 regularization. used lasso. cv Whether perform cross-validation find best lambda.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/negative_log_likelihood.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Solve logistic/poisson regression using Gradient Descent Extension to the\nmultivariate case\nThis function is not meant to be called directly by the user. — negative_log_likelihood","text":"Negative log likelihood corresponding data given   family.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/plot-fastcpd-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the data and the change points for a fastcpd object — plot,fastcpd-method","title":"Plot the data and the change points for a fastcpd object — plot,fastcpd-method","text":"Plot data change points fastcpd object","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/plot-fastcpd-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the data and the change points for a fastcpd object — plot,fastcpd-method","text":"","code":"# S4 method for fastcpd plot(x)"},{"path":"http://www.xingchi.li/fastcpd/reference/plot-fastcpd-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the data and the change points for a fastcpd object — plot,fastcpd-method","text":"x fastcpd object.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/print-fastcpd-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Print the call and the change points for a fastcpd object — print,fastcpd-method","title":"Print the call and the change points for a fastcpd object — print,fastcpd-method","text":"Print call change points fastcpd object","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/print-fastcpd-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print the call and the change points for a fastcpd object — print,fastcpd-method","text":"","code":"# S4 method for fastcpd print(x)"},{"path":"http://www.xingchi.li/fastcpd/reference/print-fastcpd-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print the call and the change points for a fastcpd object — print,fastcpd-method","text":"x fastcpd object.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/show-fastcpd-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show the available methods for a fastcpd object — show,fastcpd-method","title":"Show the available methods for a fastcpd object — show,fastcpd-method","text":"Show available methods fastcpd object","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/show-fastcpd-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show the available methods for a fastcpd object — show,fastcpd-method","text":"","code":"# S4 method for fastcpd show(object)"},{"path":"http://www.xingchi.li/fastcpd/reference/show-fastcpd-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show the available methods for a fastcpd object — show,fastcpd-method","text":"object fastcpd object.","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/summary-fastcpd-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show the summary of a fastcpd object — summary,fastcpd-method","title":"Show the summary of a fastcpd object — summary,fastcpd-method","text":"Show summary fastcpd object","code":""},{"path":"http://www.xingchi.li/fastcpd/reference/summary-fastcpd-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show the summary of a fastcpd object — summary,fastcpd-method","text":"","code":"# S4 method for fastcpd summary(object)"},{"path":"http://www.xingchi.li/fastcpd/reference/summary-fastcpd-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show the summary of a fastcpd object — summary,fastcpd-method","text":"object fastcpd object.","code":""},{"path":"http://www.xingchi.li/fastcpd/news/index.html","id":"fastcpd-023","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.3","title":"fastcpd 0.2.3","text":"Add missing examples linear regression LASSO.","code":""},{"path":"http://www.xingchi.li/fastcpd/news/index.html","id":"fastcpd-022","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.2","title":"fastcpd 0.2.2","text":"Add examples illustrate use fastcpd function. Indicating internal functions users use .","code":""},{"path":"http://www.xingchi.li/fastcpd/news/index.html","id":"fastcpd-021","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.1","title":"fastcpd 0.2.1","text":"Add examples README.","code":""},{"path":"http://www.xingchi.li/fastcpd/news/index.html","id":"fastcpd-020","dir":"Changelog","previous_headings":"","what":"fastcpd 0.2.0","title":"fastcpd 0.2.0","text":"Added NEWS.md file track changes package.","code":""}]
